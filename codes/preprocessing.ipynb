{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adaa0b36",
   "metadata": {},
   "source": [
    "**Riyadh data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# STEP 1: Load dataset\n",
    "data = pd.read_csv(r'D:\\Projects\\quantum forecasting\\dataset\\riyadh data\\cleaned_weather_data.csv')\n",
    "\n",
    "# STEP 2: Create date column from year and julian day\n",
    "data['date'] = pd.to_datetime(data['year'].astype(str), format='%Y') + pd.to_timedelta(data['julian_day'] - 1, unit='D')\n",
    "\n",
    "# STEP 3: Drop irrelevant columns\n",
    "data.drop(columns=[\"site_id\", \"max_temp_fahrenheit\", \"min_temp_fahrenheit\"], inplace=True)\n",
    "\n",
    "# STEP 4: Feature engineering\n",
    "data[\"mean_temp_celsius\"] = (data[\"max_temp_celsius\"] + data[\"min_temp_celsius\"]) / 2\n",
    "data[\"temp_range\"] = data[\"max_temp_celsius\"] - data[\"min_temp_celsius\"]\n",
    "data.drop(columns=[\"max_temp_celsius\", \"min_temp_celsius\"], inplace=True)\n",
    "\n",
    "# STEP 5: Seasonality encoding - keep and add more time features\n",
    "data[\"year_days\"] = np.where((data[\"year\"] == 2012) | (data[\"year\"] == 2016), 366, 365)\n",
    "data[\"sin_day\"] = np.sin(2 * np.pi * data[\"julian_day\"] / data[\"year_days\"])\n",
    "data[\"cos_day\"] = np.cos(2 * np.pi * data[\"julian_day\"] / data[\"year_days\"])\n",
    "\n",
    "# Additional time features for \n",
    "data['month'] = data['date'].dt.month\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "\n",
    "data.drop(columns=[\"julian_day\", \"year_days\"], inplace=True)\n",
    "\n",
    "# STEP 6: Remove missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# STEP 7: Split train/test\n",
    "train_data = data[data[\"year\"] <= 2014].copy()\n",
    "test_data = data[data[\"year\"] > 2014].copy()\n",
    "\n",
    "# STEP 8: Define feature columns\n",
    "feature_cols = [c for c in data.columns if c not in [\"year\", \"solar_radiation_langley\", \"date\"]]\n",
    "\n",
    "# Separate features to scale and those to keep as-is\n",
    "feature_cols_to_scale = [c for c in feature_cols if c not in ['sin_day', 'cos_day']]\n",
    "feature_cols_no_scale = ['sin_day', 'cos_day']\n",
    "\n",
    "# STEP 9: Scale features (excluding sin_day and cos_day)\n",
    "scaler_X = StandardScaler()\n",
    "train_scaled_array = scaler_X.fit_transform(train_data[feature_cols_to_scale])\n",
    "test_scaled_array = scaler_X.transform(test_data[feature_cols_to_scale])\n",
    "\n",
    "# STEP 10: Scale target separately\n",
    "scaler_y = StandardScaler()\n",
    "train_target_scaled = scaler_y.fit_transform(train_data[['solar_radiation_langley']])\n",
    "test_target_scaled = scaler_y.transform(test_data[['solar_radiation_langley']])\n",
    "\n",
    "# STEP 11: Replace scaled features and target in copies of original data\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "# Replace scaled feature columns\n",
    "train_data_scaled[feature_cols_to_scale] = train_scaled_array\n",
    "test_data_scaled[feature_cols_to_scale] = test_scaled_array\n",
    "\n",
    "# Keep sin_day and cos_day unchanged\n",
    "train_data_scaled[feature_cols_no_scale] = train_data[feature_cols_no_scale]\n",
    "test_data_scaled[feature_cols_no_scale] = test_data[feature_cols_no_scale]\n",
    "\n",
    "# Replace scaled target column\n",
    "train_data_scaled['solar_radiation_langley'] = train_target_scaled\n",
    "test_data_scaled['solar_radiation_langley'] = test_target_scaled\n",
    "\n",
    "# STEP 12: Save processed datasets for Informer (keep 'date' for indexing)\n",
    "train_data_scaled.to_csv(r'D:\\Projects\\quantum forecasting\\dataset\\train_data.csv', index=False)\n",
    "test_data_scaled.to_csv(r'D:\\Projects\\quantum forecasting\\dataset\\test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff966275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_id', 'year', 'julian_day', 'day_length_seconds',\n",
       "       'precipitation_mm', 'solar_radiation_langley',\n",
       "       'snow_water_equivalent_mm', 'max_temp_celsius', 'min_temp_celsius',\n",
       "       'vapor_pressure_pa', 'max_temp_fahrenheit', 'min_temp_fahrenheit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# STEP 1: Load dataset\n",
    "data = pd.read_csv(r'D:\\Projects\\quantum forecasting\\dataset\\riyadh data\\cleaned_weather_data.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f24b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
